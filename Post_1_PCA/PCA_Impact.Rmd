---
title: "How Effective is Principal Component Analysis?"
author: "Zach Schuster"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ROCR, quietly = TRUE)
require(data.table, quietly = TRUE)
require(ggplot2, quietly = TRUE)
require(randomForest, quietly = TRUE)
```


I'd like to understand how effective preprocessing a data set using PCA is for binary classification. To do this, I will use a data set with information on pulsar stars and a binary label. The data can be found [here on Kaggle](https://www.kaggle.com/pavanraj159/predicting-a-pulsar-star).

In addition, I will use the data.table package for any necessary data manipulation.

***

## Exploration

```{r read in data}
stars = fread("pulsar_stars.csv", header = TRUE)

# take a look at the data
knitr::kable(head(stars), digits = 2)
```

For more efficient use of space, I will rename the columns

```{r}
new_names = c("mean_IP","sd_IP","ek_IP","skew_IP",
              "mean_DM-SNR", "sd_DM-SNR", "ek_DM-SNR", "skew_DM-SNR",
              "target")
setnames(stars, names(stars), new_names)
```


It looks like we have 8 numeric columns of predictor variables. When looking further, there are two sets of 4 columns that take measurements on the "integrated profile" and the "DM-SNR". It is possible that variables are correlated with eachother. 

```{r plot pairs}
GGally::ggpairs(stars[, !"target"])
```

As we expected, measures related to either the integrated profile or DM-SNR are highly correlated while their is not much correlation between measures on integrated profile and measures on SM-SNR. 

This may be a good scenario for PCA!

***

### Class Distribution

Something we should note, and an added complication is the imbalanced class distribution.

```{r}
table(stars$target)

round(table(stars$target)/length(stars$target), 3)*100
```

Here we see that only 9.2% of observations are actually pulsar stars! This provides an opportunity test out some sort of sampling.

## PCA

For the sake of learning, let's manually run PCA (except for the eigen decomposition!) and project into 8 dimensions (the same as the original data set).

```{r compute PCA}
# split data into target and covariates
target = stars[["target"]]
X = stars[, !"target"]

# compute covariance matrix
cov_X = cov(X)

# compute eigen vals and eigen vectors
eigens = eigen(cov_X)

PCA = as.data.table(cbind(scale(X, scale = FALSE) %*% eigens$vectors, target))
setnames(PCA, names(PCA), c(paste("pc", 1:8, sep = "_"), "target"))

# take a look at the head of our new data set
knitr::kable(head(PCA), digits = 2,
             caption = "Principle Components")
```

## Cumulative Variance Explained by Component

We can build a nice plot to visualize how much variation is explained by each component.

```{r pareto chart}

norm_eig_values = eigens$values/sum(eigens$values)
cum_sum = cumsum(norm_eig_values)


ggplot(mapping = aes(x = 1:length(norm_eig_values), y = norm_eig_values)) +
    geom_bar(stat = "identity", fill = "orange") + 
    geom_point(aes(y = cum_sum)) + 
    geom_line(aes(y = cum_sum), col = "blue") +
    labs(title = "Variation Explained by Principal Component",
         x = "Principal Component",
         y = "Percent of Total Variation")

```

This is a nice visualization to see that we can explain about 95% of the variation with just two principal components!

***

A fun side note. By construction, PCA creates orthogonal features, which means our covariance matrix of the newly contructed features should be a diagonal matrix. Is this the case?

```{r}
round(cov(PCA[, !"target"]), 3)
```

It is!

***

Before we make any predictions, it'd be interesting to visualize components to see how seperable the classes are.

because we have almost 18,000 observations, I'll take a sample of each class to simulate the class distribution on a smaller scale

```{r}
pos = PCA[target == 1]
neg = PCA[target == 0]

# lets sample 100 data points
num_samples = 100
pos = pos[sample(1:nrow(pos), .09*num_samples)]
neg = neg[sample(1:nrow(neg), .91*num_samples)]
reduced_PCA = rbindlist(list(pos, neg))

ggplot(reduced_PCA, aes(x = pc_1, y = pc_2)) +
    geom_point(aes(col = factor(target))) +
    ggtitle("PC1 and PC2 with Class Overlayed")

```

Interestingly, there doesn't appear to be much of a linear separation (or non linear for that matter) between the classes. **Hopefully this improves when we do some sampling later on. keep this?**

```{r clear unwanted items, include=FALSE}
rm(list = ls()[!ls() %in% c("new_names", "stars")])
```


### Building training and test sets

The process is as follows

* split data into training and testing sets (with even target distribution)
* compute projection matrix for training set and transform training set
* transform testing set using computed projection matrix from previous step

```{r make train, test}
# training set proportion
prop = .7

pos = stars[target == 1]
neg = stars[target == 0]
pos_ind = sample.int(nrow(pos), round(prop*nrow(pos)))
neg_ind = sample.int(nrow(neg), round(prop*nrow(neg)))

train = rbind(pos[pos_ind], neg[neg_ind])
train[, target := as.factor(target)]

test = rbind(pos[-pos_ind], neg[-neg_ind])
test[, target := as.factor(target)]
```

We have our training and testing sets which we can use for the duration of the analysis.

### PCA vs full features using Random Forest

```{r compute projection matrix on train}
# number of pricipal components that you'd like to compute
num_pcs = 5

train_x = train[, !"target"]
train_y = train[, "target"]

# compute covariance
train_cov = cov(train_x)

# compute eigen decomposition on covariance matrix
eigen_train = eigen(train_cov)

scaled_train = scale(train_x, scale = FALSE)
train_pca = scaled_train %*% eigen_train$vectors[, 1:num_pcs]

train_pca = cbind(train_pca, train_y)
setnames(train_pca, names(train_pca)[1:num_pcs],
         paste("pc", 1:num_pcs, sep = "_"))

knitr::kable(head(train_pca), digits = 2, 
             caption = "Projected Training Set")
```

Above we have our transformed training set. Next we transform our testing set.

```{r transform test}
test_x = test[, !"target"]
test_y = test[, .(target)]

transformed_test = scale(test_x, center = attr(scaled_train, "scaled:center"),
                       scale = FALSE) %*% eigen_train$vectors[, 1:num_pcs]

test_pca = cbind(transformed_test, test_y)
test_pca[, target := as.factor(target)]

setnames(test_pca, names(test_pca)[1:num_pcs],
         paste("pc", 1:num_pcs, sep = "_"))

knitr::kable(head(test_pca), digits = 2,
             caption = "Projected Test Set")
```

Great! Now we have our training and test sets projected onto a new feature space. We've used the first two principal components here.

***

Now we can train our random forest!

```{r RF train no sampling}

RF_pca_no_samp = randomForest(x = train_pca[, !"target"],
                              y = train_pca[, target],
                              ntree = 1000, importance = TRUE)

varImpPlot(RF_pca_no_samp, type = 2,
           main = "Decrease in Gini")
```

Above we have a simple plot showing that, to no surprise, pc_1 was the more important feature between pc_1 and pc_2. 

***

We can now test our random forest trained on transformed data.

```{r test RF pca}

preds_pca = predict(RF_pca_no_samp, newdata = test_pca[, !"target"],
                    type = "prob")[, 2]

# get performance of model
pred_obj = prediction(preds_pca, test_pca[, target])
c(AUC = performance(pred_obj, measure = "auc")@y.values[[1]])
# as.numeric(as.character(test_pca[, target]))
```

An AUC of .94 is really good! We don't know how good that is until we compare it to a data set using all of the features. **For comparison, we will use the exact same training and testing observations**.

```{r train RF no samp}

RF_no_samp = randomForest(x = train[, !"target"],
                          y = train[, target],
                          ntree = 1000, importance = TRUE)

varImpPlot(RF_no_samp, type = 2,
           main = "Decrease in Gini")
```

Here we can see that the most important feature is the excess kurtosis of the integrated profile.

```{r test RF no samp}
preds = predict(RF_no_samp, newdata = test[, !"target"],
                type = "prob")[, 2]

# get performance of model
pred_obj = prediction(preds, test[, target])
c(AUC = performance(pred_obj, measure = "auc")@y.values[[1]])

```

An AUC of .97 is exceptional. That being said, using only 2 features instead of 8, we were able to get an AUC within .03 (.97-.94) of the model built on the entire data set! 

This is a great demonstration of the power of PCA for the purpose of dimension reduction.
